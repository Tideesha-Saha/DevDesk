#DEVDESK.PY
import streamlit as st
import PyPDF2
from EXT_UI import process_pdf_text
from PIL import Image
from crew_b import run_business_analyst
from crew_d import run_design_agent
from crew_dev import run_developer_agent
from crew_test import run_tester_agent

def local_css(file_name):
    with open(file_name) as f:
        st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)

local_css("styleDEVDESK.css")

logo = Image.open("logo_transparent.png")
st.image(logo, width=150)
# st.subheader("Autonomous dev pods. Seamless delivery. Maximum impact.")

# st.markdown("---")
# st.title("Initiate Your Project – Upload RFP")

st.markdown("""
    <h1 id="Upload-h1" style="font-size: 2.5rem;color: #1d547b;text-align: center;">Initiate Your Project – <span id="RFP-span-id">Upload RFP</span></h1>
            """,unsafe_allow_html=True)
st.markdown("""
            <p style="text-align:center" id="subtext">Upload your Request for Proposal (RFP) document here to initiate your project with DevDesk. Our autonomous dev pods will review your requirements and get started immediately. Ensure the file is in PDF format for accurate processing. Let’s turn your ideas into impactful, seamless software solutions.</p>
            """, unsafe_allow_html=True)

uploaded_file = st.file_uploader("", type=["pdf"])

if uploaded_file:
    # Extract text from PDF file object
    pdf_text = ""
    pdf_reader = PyPDF2.PdfReader(uploaded_file)
    for page in pdf_reader.pages:
        pdf_text += page.extract_text() + "\n"

    st.success("PDF uploaded and text extracted successfully!")

    # Save to session_state
  


    # if st.button("Process Text"):
    #     with st.spinner("Analyzing..."):
    #         requirements, tfidf_vectors = process_pdf_text(pdf_text)
    #         st.session_state.requirements = "\n".join(requirements)

    if st.button("Initialize Project"):
        
        with st.spinner("Analyzing..."):
            # Step 1: Extract requirements
            requirements, tfidf_vectors = process_pdf_text(pdf_text)
            st.session_state.requirements = "\n".join(requirements)

            # with open("extracted_reqmts.txt", "w", encoding="utf-8") as f:
            #     f.write("\n".join(requirements))

            with open("extracted_reqmts.txt", "w", encoding="utf-8") as f:
                for req in requirements:
                    f.write(req + "\n")
            
            
            
            
            

            # Step 2: Send to Business Analyst Agent
            # ba_output = run_business_analyst("\n".join(requirements))
            # st.session_state.ba_output = ba_output

        # st.subheader("📌 ")
        # with st.expander("Extracted Requirements", expanded=False):
        #     if requirements:
        #         for i, req in enumerate(requirements, 1):
        #             st.markdown(f"**{i}.** {req}")
        #     else:
        #         st.warning("No clear requirements found.")

            # st.subheader("🔢 TF-IDF Vectors (Preview)")
            # if len(tfidf_vectors):
            #     st.write(tfidf_vectors[:5])  # show first 5 vectors
            # else:
            #     st.write("No vectors generated.")

        
            


        # === Display Agent Output ===
        # st.subheader("📋 User Stories (Generated by Business Analyst Agent)")
        # st.text_area("Business Analyst Agent Output:", ba_text, height=300)
        
        import time

        st.markdown("---")

        st.subheader("Business Analyst Agent")
        with st.spinner("Generating Business Analyst Output..."):
            start_time = time.time()
            # result = crew_b.run_business_analyst(user_input)
            ba_text = run_business_analyst()
            end_time = time.time()
            generation_time = end_time - start_time
           
            st.markdown(f"Generation time: {generation_time:.2f} seconds.  <span style='color:green;'> Generated Output ✅</span>", unsafe_allow_html=True)
            ba_output = f'<div style="border: 3px solid #b4d6e3;text-align:left; padding: 20px; border-radius: 5px; color: black; background-color: white; width: 100%;">{ba_text}</div>'

        with st.expander("User Stories", expanded=False):
            st.subheader("Developer Agent Output:")
            st.markdown(ba_output,unsafe_allow_html=True)




        st.subheader("Design Agent")
        with st.spinner("Generating Design Agent Output..."):
            start_time = time.time()
            # result = crew_d.run_design_agent()
            design_text = run_design_agent()
            end_time = time.time()
            generation_time = end_time - start_time
           
            st.markdown(f"Generation time: {generation_time:.2f} seconds.  <span style='color:green;'> Generated Output ✅</span>", unsafe_allow_html=True)
            da_output = f'<div style="border: 3px solid #b4d6e3; padding: 10px; border-radius: 5px; color: black; background-color: white; width: 100%;">{design_text}</div>'

        with st.expander("System Design and Architecture", expanded=False):
            st.subheader("Design Agent Output:")
            st.markdown(da_output, unsafe_allow_html=True)


        # with st.expander("System Design & Architecture", expanded=False):
        #     st.subheader("Design Agent Output:")
        #     st.markdown(design_text,unsafe_allow_html=True)
        # 
        #  
        st.subheader("Developer Agent")
        with st.spinner("Generating Developer Agent Output..."):
            start_time = time.time()
            # result = crew_dev.run_developer_agent()
            code = run_developer_agent()
            end_time = time.time()
            generation_time = end_time - start_time
            # st.write(f"Generation time: {generation_time:.2f} seconds")
            st.markdown(f"Generation time: {generation_time:.2f} seconds.  <span style='color:green;'> Generated Output ✅</span>", unsafe_allow_html=True)
            # dev_output = f'<div style="border: 3px solid #b4d6e3; padding: 10px; border-radius: 5px; color: black; background-color: white; width: 100%;">{code}</div>'
            # dev_output = {code}

        with st.expander("Production-ready Codebase", expanded=False):
            st.subheader("Developer Agent Output:")
            st.code(code)

       
       
       
       
       
       
        # with st.expander("Production-ready Codebase", expanded=False):
        #     st.subheader("Developer Agent Output:")
        #     st.markdown(code,unsafe_allow_html=True) 

        st.subheader("Tester Agent")
        with st.spinner("Generating Tester Agent Output..."):
            start_time = time.time()
            test_cases = run_tester_agent()
            # code = run_developer_agent()
            end_time = time.time()
            generation_time = end_time - start_time
            
            st.markdown(f"Generation time: {generation_time:.2f} seconds.  <span style='color:green;'> Generated Output ✅</span>", unsafe_allow_html=True)
            # tester_output = {test_cases}

        with st.expander("Test Cases", expanded=False):
            st.subheader("Tester Agent Output:")
            st.code(test_cases)



        # with st.expander("Test Cases", expanded=False):
        #     st.subheader("Tester Agent Output:")
        #     st.markdown(test_cases,unsafe_allow_html=True) 




        # st.text_area("Design Agent Output:", design_text, height=300)
        # st.text_area("Developer Agent Output:", code, height=300)
        # st.text_area("Tester Agent Output:", test_cases, height=300)

        # Optional Download Button



        # 📂📂📂USER Stories File
        # st.download_button(
        #     label="📄 Download User Stories",
        #     data=ba_text,
        #     file_name="User_Stories.txt",
        #     mime="text/plain"
        # )




#CHATBOT🤖🤖 working abs fine
# from datetime import datetime
# from project_status import project_status

# st.title("Project Progress Chatbot")

# user_query = st.text_input("Ask about your project status...")

# if user_query:
#     st.markdown("**You:** " + user_query)

#     # Simple chatbot response logic
#     if "requirement" in user_query.lower():
#         if project_status['requirements_extracted'] == 0:
#             response = "Sorry, no requirements have been extracted yet."
#         else:
#             response = f"**{project_status['requirements_extracted']}** requirements have been extracted so far."

#     elif "user stories" in user_query.lower():
#         if project_status['user_stories_generated'] == 0:
#             response = "Sorry, no user stories have been generated yet."
#         else:
#             response = f"**{project_status['user_stories_generated']}** user stories have been generated so far."

#     elif "test case" in user_query.lower():
#         if project_status['test_cases_generated'] == 0:
#             response = "Sorry, no test cases have been generated yet."
#         else:
#             response = f"**{project_status['test_cases_generated']}** test cases have been generated so far."

#     elif "design" in user_query.lower():
#         status = "generated" if project_status["design_document_generated"] else "is not generated yet"
#         response = f"The design document has **{status}**."

#     elif (
#         "summary" in user_query.lower()
#         or "status" in user_query.lower()
#         or "progress" in user_query.lower()
#         or "work done" in user_query.lower()
#         or "what has been done do far" in user_query.lower()
#         or "everything done so far" in user_query.lower()
#         or "everything completed so far" in user_query.lower()
#     ):
#         design = "Yes" if project_status["design_document_generated"] else "No"
#         response = (
#             f"**Project Summary:**\n"
#             f"- Requirements: {project_status['requirements_extracted']}\n"
#             f"- User Stories: {project_status['user_stories_generated']}\n"
#             f"- Test Cases: {project_status['test_cases_generated']}\n"
#             f"- Design Document Generated: {design}"
#         )

#     elif "work left" in user_query.lower() or "how much work is left" in user_query.lower() or "left" in user_query.lower() or "remaining" in user_query.lower():
#         response_parts = []

#         # Requirements
#         if project_status["requirements_extracted"] == 0:
#             response_parts.append("Requirements are yet to be extracted.")
#         else:
#             response_parts.append(f"{project_status['requirements_extracted']} requirements have been extracted.")

#         # User Stories
#         if project_status["user_stories_generated"] == 0:
#             response_parts.append("\nUser stories are yet to be generated.")
#         else:
#             response_parts.append(f"{project_status['user_stories_generated']} user stories have been generated.")

#         # Test Cases
#         if project_status["test_cases_generated"] == 0:
#             response_parts.append("No test cases have been generated yet.")
#         else:
#             response_parts.append(f"{project_status['test_cases_generated']} test cases have been created.")

#         # Design Document
#         if not project_status["design_document_generated"]:
#             response_parts.append("Design document is pending.")
#         else:
#             response_parts.append("Design document has been completed.")

#         # Deployment
#         if not project_status["deployment_done"]:
#             response_parts.append("Deployment is still pending.")
#         else:
#             response_parts.append("Deployment has been completed.")

#         # Join all messages
#         response = " ".join(response_parts)

#     else:
#         response = "Sorry, I didn’t understand that. Try asking about 'requirements', 'test cases', or 'design'."

#     st.markdown("**Bot:** " + response)



# #chatbot using GEMINI🤖🤖✌️ Working fine too
# from datetime import datetime
# from project_status import project_status
# import streamlit as st
# import google.generativeai as genai  # Gemini LLM
# genai.configure(api_key="AIzaSyB2XPTXRP6PyiAz-Vr9g0ZY_JTIIJtLSiE")  # Replace with your actual Gemini API key

# st.title("Project Progress Chatbot")

# # user_query = st.text_input("Ask about your project status...")

# def format_project_status(status: dict) -> str:
#     return (
#         f"- Requirements Extracted: {status['requirements_extracted']}\n"
#         f"- User Stories Generated: {status['user_stories_generated']}\n"
#         f"- Test Cases Generated: {status['test_cases_generated']}\n"
#         f"- Design Document Generated: {'Yes' if status['design_document_generated'] else 'No'}\n"
#         f"- Deployment Done: {'Yes' if status['deployment_done'] else 'No'}"
#     )

# # if user_query:
# #     st.markdown("**You:** " + user_query)



# #     # Combine the project status with the user's question
# #     prompt = f"""
# #     You are a helpful assistant that answers queries about a software project's progress.

# #     Here is the current project status:
# #     {format_project_status(project_status)}

# #     User's question: "{user_query}"

# #     Answer considering the above project status.
# #     """

# #     with st.spinner("Thinking..."):
# #         model = genai.GenerativeModel('models/gemini-1.5-pro')
# #         response = model.generate_content(prompt)
        
# #     st.markdown("**Bot:** " + response.text)


# # import streamlit as st
# # import genai  # Assuming this is properly set up

# # Initialize session state to hold chat history
# if "chat_history" not in st.session_state:
#     st.session_state.chat_history = []

# # Input from the user
# user_query = st.text_input("Ask something about the project:")

# if user_query:
#     # Show the user's message
#     st.session_state.chat_history.append(("You", user_query))

#     # Create prompt
#     prompt = f"""
#     You are a helpful assistant that answers queries about a software project's progress.

#     Here is the current project status:
#     {format_project_status(project_status)}

#     User's question: "{user_query}"

#     Answer considering the above project status.
#     """

#     with st.spinner("Thinking..."):
#         model = genai.GenerativeModel('models/gemini-1.5-pro')
#         response = model.generate_content(prompt)

#     # Save bot's response
#     st.session_state.chat_history.append(("Bot", response.text))
#     # st.session_state.chat_history.insert(0, ("Bot", response.text))

# # Display chat history
# # for speaker, message in st.session_state.chat_history:
# #     st.markdown(f"**{speaker}:** {message}")


# for speaker, message in st.session_state.chat_history:
#     if speaker == "You":
#         st.markdown(f"<p style=' width:fit-content; background-color:#e6f5ff; border-radius: 10px; padding:5px; margin-bottom:8px; margin-left:5vw;'>You: {message}</p>", unsafe_allow_html=True)
#     else:
#         st.markdown(f"<div style='border: 1.2px solid #1d547b; border-radius: 10px; padding:5px; margin-bottom:40px; margin-left:5vw;'>Bot: {message}</div>", unsafe_allow_html=True)